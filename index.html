<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Security Observer | Dennis Kontic</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header class="site-header">
        <h1>AI Security Observer</h1>
        <p class="subtitle">Tracking the intersection of AI, Cybersecurity, and Red Teaming</p>
        <nav>
            <ul class="nav-links">
                <li><a href="index.html" class="active">Home</a></li>
                <li><a href="resources.html">Resources</a></li>
                <li><a href="about.html">About</a></li>
            </ul>
        </nav>
    </header>

    <main class="container">
        <section class="featured-post">
            <div class="featured-content">
                <span class="badge badge-red">Red Team</span>
                <span class="badge badge-capstone">Capstone</span>
                
                <h2><a href="posts/red-teaming-llms.html">How to Red Team Large Language Models: A Guide to Basic Testing</a></h2>
                <p class="date">February 20, 2026</p>
                
                <div class="bluf-box">
                    <strong>Bottom Line Up Front (BLUF):</strong> While Blue Team defenses focus on securing the perimeter, Red Teaming LLMs requires an adversarial mindset to expose internal logic flaws. This guide breaks down prompt injection testing and safety evaluations.
                </div>
                
                <p class="summary">A deep-dive technical analysis covering basic adversarial testing prompts, safety evaluation techniques, and how to identify vulnerabilities before threat actors do.</p>
                
                <a href="posts/red-teaming-llms.html" class="read-more">Read Full Guide &rarr;</a>
            </div>
        </section>

        <section class="archive-section">
            <h3 class="section-title">Latest Updates</h3>
            <div class="post-list">
                
                <article class="post-item">
                    <span class="badge badge-blue">Blue Team</span>
                    <h4><a href="posts/prompt-injection-guide.html">The Blue Team's Guide to LLM Attacks: Distinguishing Prompt Injection from Jailbreaking</a></h4>
                    <p class="date">February 12, 2026</p>
                </article>

                <article class="post-item">
                    <span class="badge badge-blue">Blue Team</span>
                    <h4><a href="posts/automated-defenses.html">Beyond the Script: Scaling Blue Team Defenses with AI</a></h4>
                    <p class="date">February 1, 2026</p>
                </article>

                <article class="post-item">
                    <span class="badge badge-case">Case Study</span>
                    <h4><a href="posts/shadow-ai.html">The Rise of "Shadow AI": When Employees Bypass Security</a></h4>
                    <p class="date">January 27, 2026</p>
                </article>

                <article class="post-item">
                    <span class="badge badge-blue">Compliance</span>
                    <h4><a href="posts/nist-generative-ai.html">NIST Generative AI Profile: Standards for Secure AI</a></h4>
                    <p class="date">January 15, 2026</p>
                </article>

            </div>
        </section>
    </main>

    <footer>
        <p>&copy; 2026 Dennis Kontic. Built for Lindenwood University Applied AI.</p>
    </footer>
</body>
</html>
