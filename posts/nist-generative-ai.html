<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NIST Generative AI Profile - AI Security Observer</title>
    <link rel="stylesheet" href="../styles.css">
    <style>
        .post-image {
            width: 100%;
            height: auto;
            border-radius: 8px;
            margin-bottom: 10px;
            border: 1px solid #ddd;
        }
        .caption {
            font-style: italic;
            font-size: 0.9em;
            color: #666;
            text-align: center;
            display: block;
            margin-bottom: 20px;
        }
    </style>
</head>
<body>

    <header>
        <h1>AI Security Observer</h1>
        <p>Tracking the intersection of AI, Cybersecurity, and Red Teaming</p>
    </header>

    <nav>
        <a href="../index.html">Home</a>
        <a href="../about.html">About</a>
    </nav>

    <div class="container">
        
        <h2>NIST Generative AI Profile Sets the Standard for Secure AI</h2>
        <p class="post-date">January 20, 2026</p>
        <hr>

        <img src="../assets/images/nist-blueprint.jpg" alt="Digital blueprint of an AI chip symbolizing safety standards" class="post-image">
        <span class="caption">The new profile acts as a "building code" for AI systems.</span>

        <h3>What happened</h3>
        <p>The National Institute of Standards and Technology (NIST) has released the Generative AI Profile (NIST AI 600-1), a companion guide to their famous AI Risk Management Framework. This document was created specifically to help organizations handle the unique risks of "Generative AI" like ChatGPT or Claude rather than just general machine learning.</p>

        <h3>Details</h3>
        <p>This profile gives cybersecurity teams a checklist of 400+ specific actions to secure their AI.</p>
        <ul>
            <li><strong>Key Focus:</strong> It tells companies they need to check for things like "confabulation" (hallucinations), data privacy leaks, and how easy it is for an attacker to break the model.</li>
            <li><strong>Actionable Guidance:</strong> It doesn't just talk about theory; it gives a list of things to do, like "Identified and Managed" risks, which helps IT teams treat AI just like any other piece of software they secure.</li>
        </ul>

        <h3>Context</h3>
        <p>For a long time companies were just guessing how to secure AI. Now because NIST is the government standard this document effectively becomes the "rulebook." If a company gets hacked or sued people will look at this document to see if they were following the rules. It moves AI security from "wild west" to "standard procedure."</p>

        <h3>My Take</h3>
        <p>Think of this like a building code for AI. Before this people were just building AI houses however they wanted. Now there is an actual inspection list to make sure the house doesn't burn down. It is boring paperwork but it is exactly what big companies need to feel safe using these tools.</p>

        <p><a href="../index.html">&larr; Back to Home</a></p>

    </div>

    <footer>
        <p>&copy; 2026 Dennis Kontic. Built for Lindenwood University Applied AI.</p>
    </footer>

</body>
</html>
