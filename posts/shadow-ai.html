<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Rise of Shadow AI - AI Security Observer</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>

    <header>
        <h1>AI Security Observer</h1>
        <p>Tracking the intersection of AI, Cybersecurity, and Red Teaming</p>
    </header>

    <nav>
        <a href="../index.html">Home</a>
        <a href="../about.html">About</a>
    </nav>

    <div class="container">
        
        <h2>The Rise of "Shadow AI": When Employees Bypass Security</h2>
        <p class="post-date">January 27, 2026</p>
        <hr>

        <h3>What happened</h3>
        <p>Recent industry surveys suggest a massive surge in "Shadow AI"â€”employees using unauthorized generative AI tools (like ChatGPT, Claude, or Midjourney) to do their jobs without IT department approval or knowledge.</p>

        <h3>Details</h3>
        <p>While companies are busy setting up firewalls (like Model Armor) and reading NIST guidelines, their actual employees are bypassing these controls entirely.</p>
        <ul>
            <li><strong>Key Focus:</strong> Engineers are frequently pasting proprietary code into public chatbots to fix bugs, or HR teams are uploading resumes to summarize them, unknowingly exposing company data.</li>
            <li><strong>Actionable Guidance:</strong> Once data enters a public model, it may legally become part of the training set. Samsung notably faced this issue when engineers leaked source code to ChatGPT, proving that policy alone isn't enough to stop this behavior.</li>
        </ul>

        <h3>Context</h3>
        <p>This is the rebirth of "Shadow IT" (when employees used Dropbox before it was approved). Security teams can't just block these tools because they are too useful. If the official tools are clunky, employees will find a way to use the fast, public ones.</p>

        <h3>My Take</h3>
        <p>Security is no longer just about code. it's about user experience. If you don't provide your employees with a safe, approved, and <em>good</em> AI tool, they will use an unsafe one. The best security patch right now isn't software, it's giving your team a corporate ChatGPT license so they don't have to use their personal accounts.</p>

        <p><a href="../index.html">&larr; Back to Home</a></p>

    </div>

    <footer>
        <p>&copy; 2026 Dennis Kontic. Built for Lindenwood University Applied AI.</p>
    </footer>

</body>
</html>
